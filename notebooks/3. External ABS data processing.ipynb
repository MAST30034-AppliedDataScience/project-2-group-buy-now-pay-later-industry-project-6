{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***External ABS data handling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, regexp_replace, to_date, avg\n",
    "from scripts.download import download_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded ABS files.\n"
     ]
    }
   ],
   "source": [
    "download_abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondence = pd.read_csv(\"../data/tables/abs/postcode_correspondences_2021.csv\")\n",
    "correspondence.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postcodes may be matched to multiple SA2 regions, therefore we will choose the region with the highest ratio (percentage of population for that postcode) as representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of max ratio values for each postcode\n",
    "max_indices = correspondence.groupby(\"POSTCODE\")[\"RATIO_FROM_TO\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondence_filtered = correspondence.loc[max_indices].reset_index(drop=True)\n",
    "correspondence_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can then get rid of all columns except postcode and SA2 code, which will be of use for combining our ABS data with the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondence_filtered = correspondence_filtered[[\"POSTCODE\", \"SA2_CODE_2021\"]]\n",
    "correspondence_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assign a SA2 code for each customer in our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# data is seperated by pipe \"|\", not comma\n",
    "customers = pd.read_csv(\"../data/tables/synthetic/tbl_consumer.csv\", sep=\"|\")\n",
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_merged = pd.merge(customers, correspondence_filtered, left_on=\"postcode\", right_on=\"POSTCODE\", how=\"left\")\n",
    "customers_merged = customers_merged.drop(\"POSTCODE\", axis=1)\n",
    "customers_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_merged[\"SA2_CODE_2021\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "83,181 missing SA2 codes - maybe impute with mean/median values for state when using ABS data later? Or find similar postcodes or use latitude/longitude data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values with zeroes (helps with merging later)\n",
    "customers_merged[\"SA2_CODE_2021\"] = customers_merged[\"SA2_CODE_2021\"].fillna(0)\n",
    "customers_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABS Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_df = pd.read_csv(\"../data/tables/abs/ABS_2021.csv\")\n",
    "abs_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before filtering:\n",
    "abs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get state-wide statistics for imputation purposes later. Since we are focusing on median personal income, we will only get each state's median personal income number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = \"2: Median total personal income ($/weekly)\"\n",
    "\n",
    "NSW_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"1: New South Wales\")][\"OBS_VALUE\"])\n",
    "\n",
    "VIC_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"2: Victoria\")][\"OBS_VALUE\"])\n",
    "\n",
    "QLD_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"3: Queensland\")][\"OBS_VALUE\"])\n",
    "\n",
    "SA_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"4: South Australia\")][\"OBS_VALUE\"])\n",
    "\n",
    "WA_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"5: Western Australia\")][\"OBS_VALUE\"])\n",
    "\n",
    "TAS_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"6: Tasmania\")][\"OBS_VALUE\"])\n",
    "\n",
    "NT_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"7: Northern Territory\")][\"OBS_VALUE\"])\n",
    "\n",
    "ACT_median_income = float(abs_df[(abs_df[\"MEDAVG: Median/Average\"] == stat) &\n",
    "                           (abs_df[\"REGION: Region\"] == \"8: Australian Capital Territory\")][\"OBS_VALUE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we only want SA2 data, and data from 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered = abs_df[(abs_df[\"REGION_TYPE: Region Type\"] == \"SA2: Statistical Area Level 2\") & \n",
    "                      (abs_df[\"TIME_PERIOD: Time Period\"] == 2021)]\n",
    "abs_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert the SA2 region into only its code, and also remove unnecessary features. We will also rename the relevant columns for ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change SA2 feature\n",
    "abs_filtered[\"REGION: Region\"] = abs_filtered[\"REGION: Region\"].str[:9].astype(\"int64\")\n",
    "abs_filtered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered = abs_filtered.drop([\"DATAFLOW\", \"REGION_TYPE: Region Type\", \"STATE: State\",\n",
    "                                  \"TIME_PERIOD: Time Period\"], axis=1) # drop useless columns\n",
    "abs_filtered = abs_filtered.rename(columns={\"MEDAVG: Median/Average\": \"statistic\",\n",
    "                                            \"REGION: Region\": \"region\",\n",
    "                                            \"OBS_VALUE\": \"value\"}) # rename columns\n",
    "abs_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should engineer new columns based on the categorical values of the `Statistic` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered = abs_filtered.pivot_table(index=\"region\", columns=\"statistic\", values=\"value\", aggfunc=\"sum\")\n",
    "abs_filtered = abs_filtered.reset_index()\n",
    "abs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all columns for simplicity\n",
    "abs_filtered.columns = [\"region\", \"median_age\", \"median_personal_income\",\n",
    "                        \"median_family_income\", \"median_household_income\", \"median_mortgage\",\n",
    "                        \"median_rent\", \"avg_bedroom\", \"avg_household\"]\n",
    "abs_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered.drop(\"region\", axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove instances which include outlier values for some features. For example, we can see that minimum values for a lot of the statistics are zero, which doesn't make sense. We should also remove any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "abs_filtered = abs_filtered[(abs_filtered[\"median_age\"] > 0) & \n",
    "                            (abs_filtered[\"median_personal_income\"] > 0) &\n",
    "                            (abs_filtered[\"median_family_income\"] > 0) &\n",
    "                            (abs_filtered[\"median_household_income\"] > 0) &\n",
    "                            (abs_filtered[\"median_mortgage\"] > 0) &\n",
    "                            (abs_filtered[\"median_rent\"] > 0) &\n",
    "                            (abs_filtered[\"avg_bedroom\"] > 0) &\n",
    "                            (abs_filtered[\"avg_household\"] > 0)]\n",
    "\n",
    "abs_filtered = abs_filtered.dropna() # remove NaN values\n",
    "\n",
    "abs_filtered.drop(\"region\", axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions of these numeric features now look much more sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external dataset shape after filtering\n",
    "abs_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the median personal income (our variable of interest) and merge this with our customer dataset according to SA2 region code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_personal_income_df = abs_filtered.loc[:, [\"region\", \"median_personal_income\"]]\n",
    "customers_merged[\"SA2_CODE_2021\"] = customers_merged[\"SA2_CODE_2021\"].astype(\"int\") # helps with merging\n",
    "\n",
    "df_merged = pd.merge(customers_merged, median_personal_income_df, left_on=\"SA2_CODE_2021\", right_on=\"region\", how=\"left\")\n",
    "df_merged = df_merged.drop(\"SA2_CODE_2021\", axis=1) # drop duplicate column\n",
    "df_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are missing some income numbers (see the `NaN` value), we will impute each missing value with the median income of the state where the customer is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_values = {\"NSW\": NSW_median_income, # values determined through initial ABS dataset\n",
    "                      \"VIC\": VIC_median_income,\n",
    "                      \"QLD\": QLD_median_income,\n",
    "                      \"SA\": SA_median_income,\n",
    "                      \"WA\": WA_median_income,\n",
    "                      \"TAS\": TAS_median_income,\n",
    "                      \"NT\": NT_median_income,\n",
    "                      \"ACT\": ACT_median_income}\n",
    "\n",
    "replacement_series = df_merged[\"state\"].map(replacement_values)\n",
    "\n",
    "df_merged[\"median_personal_income\"] = df_merged[\"median_personal_income\"].fillna(replacement_series)\n",
    "df_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"median_personal_income\"].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our median personal income column now has no missing values, and hence the data has been properly imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the folder path and filename\n",
    "output_path = \"../data/curated/\"\n",
    "file_name = \"consumers_median_income\"\n",
    "\n",
    "# create the folder if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# save df to csv in the specified folder\n",
    "file_path = os.path.join(output_path, file_name)\n",
    "df_merged.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Dataframe\n",
    "\n",
    "Each column: Order ID, Date, ABN, User ID, Consumer ID, Fraud Probability, Median Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"nathan\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_fraud = pd.read_csv(\"../data/tables/synthetic/consumer_fraud_probability.csv\")\n",
    "consumer_details = spark.read.parquet(\"../data/tables/synthetic/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_details.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_path = \"../data/tables/synthetic/transactions/\"\n",
    "\n",
    "# Read all parquet files and add a new column for the file name\n",
    "df = spark.read.parquet(transactions_path).withColumn(\"order_datetime\", input_file_name())\n",
    "df = df.withColumn(\"order_datetime\", regexp_extract(input_file_name(), r'[^/]+$', 0))\n",
    "\n",
    "df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `date` column into date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"order_datetime\", regexp_replace(\"order_datetime\", r'\\.parquet$', ''))\n",
    "df = df.withColumn(\"order_datetime\", to_date(\"order_datetime\", \"yyyy-MM-dd\"))\n",
    "\n",
    "df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add fraud probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_fraud_spark = spark.createDataFrame(consumer_fraud)\n",
    "consumer_fraud_spark.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicates in the fraud probability dataframe (i.e. entries with the same `user_id` and `order_datetime`), so we will average out all the `fraud_probability` values over the duplicates, and remove any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_fraud_spark = consumer_fraud_spark.groupBy(\"user_id\", \"order_datetime\").agg(\n",
    "    avg(\"fraud_probability\").alias(\"avg_fraud_prob\")\n",
    ")\n",
    "\n",
    "consumer_fraud_spark.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df.join(consumer_fraud_spark, on=[\"user_id\", \"order_datetime\"], how=\"left\")\n",
    "df_fraud = df_fraud.fillna({\"avg_fraud_prob\": 0})\n",
    "\n",
    "df_fraud.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add in the consumer IDs from the `consumer_details` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_details.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df_fraud.join(consumer_details, on=[\"user_id\"], how=\"left\")\n",
    "df_fraud.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud.filter(df_fraud[\"consumer_id\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are no missing values in the `consumer_id` column, so there are no issues with merging. Now, we can merge again using our `df_merged` dataset, which contains median income data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert median income pandas df to a spark df\n",
    "median_income_spark = spark.createDataFrame(df_merged[[\"consumer_id\", \"median_personal_income\"]])\n",
    "median_income_spark.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_fraud.join(median_income_spark, on=[\"consumer_id\"], how=\"left\")\n",
    "df_final.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.filter(df_final[\"median_personal_income\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our new column has no missing values, so our dataframe is complete. Next step is to add the take rate depending on `merchant_abn`, and calculate purchase power, expected revenue and expected loss for each transaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
